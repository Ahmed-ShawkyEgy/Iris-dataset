{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Iris DataSet</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading the dataset</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Load the iris dataset and its corresponding data and targets</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into the variable x\n",
    "X = iris.data\n",
    "\n",
    "# Load the target into the variable y\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into two sets, one for training and one for testing for later use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "\n",
    "# use train/test split with different random_state values \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Learning data using LinearSVC Model </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Choosing paramaters using grid search</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We are going to brute force the hyperparamaters of the LinearSVC model using the gridSearch class<br>First, we need to choose the set of hyper-paramaters for the grid to bruteforce upon<br>Then, we pass these paramters along with the model type to the grid class.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1, 10, 100, 300, 500, 700, 1000], 'tol': [0.0001, 1e-05, 1e-06]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# choose possible paramaters\n",
    "param_grid = {\n",
    "                'C':[1,10,100,300,500,700,1000],\n",
    "                'tol':[1e-4,1e-5,1e-6]\n",
    "             }\n",
    "\n",
    "# instantiate the grid\n",
    "# the cv param indicates the number of folds and the scoring param indicates the scoring stratetgy\n",
    "grid = GridSearchCV(LinearSVC(random_state=1), param_grid, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "\n",
    "# fit the grid with data\n",
    "# note that we don't need to do any splitting since the gridSearch does that for us\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next, we print out the results</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_test_score: [ 0.96666667  0.96666667  0.96666667  0.94666667  0.94        0.96        0.94\n",
      "  0.92666667  0.96        0.93333333  0.94        0.88666667  0.89333333\n",
      "  0.87333333  0.93333333  0.9         0.88666667  0.92666667  0.90666667\n",
      "  0.87333333  0.88666667] \n",
      "\n",
      "std_test_score: [ 0.0421637   0.0421637   0.0421637   0.06182412  0.04898979  0.03887301\n",
      "  0.05734884  0.08793937  0.03265986  0.03651484  0.05734884  0.07483315\n",
      "  0.09285592  0.06798693  0.04714045  0.0843274   0.05811865  0.04898979\n",
      "  0.09043107  0.06798693  0.10873004] \n",
      "\n",
      "mean_train_score: [ 0.96666667  0.96666667  0.96833333  0.955       0.94166667  0.95666667\n",
      "  0.93333333  0.94833333  0.94666667  0.935       0.94666667  0.89333333\n",
      "  0.91        0.91833333  0.93333333  0.90166667  0.92333333  0.92833333\n",
      "  0.90666667  0.925       0.885     ] \n",
      "\n",
      "std_train_score: [ 0.01178511  0.01178511  0.01105542  0.01545603  0.00912871  0.01779513\n",
      "  0.04013865  0.03265986  0.04459696  0.03045944  0.02718251  0.07644897\n",
      "  0.05661763  0.02758824  0.01972027  0.06506407  0.02953341  0.03674235\n",
      "  0.06879922  0.03073181  0.10060926]\n"
     ]
    }
   ],
   "source": [
    "# print the mean scores of the 12 different combinations of hyperparamaters over 10 folds\n",
    "\n",
    "print(\"mean_test_score:\",grid.cv_results_['mean_test_score'],'\\n')\n",
    "print(\"std_test_score:\",grid.cv_results_['std_test_score'],'\\n')\n",
    "print(\"mean_train_score:\",grid.cv_results_['mean_train_score'],'\\n')\n",
    "print(\"std_train_score:\",grid.cv_results_['std_train_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Then we show the best hyper-paramaters for our model along with the best scores </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best paramaters :  {'C': 1, 'tol': 0.0001}\n",
      "best score :  0.966666666667\n"
     ]
    }
   ],
   "source": [
    "# print the best hyper-paramaters\n",
    "print(\"best paramaters : \" ,grid.best_params_)\n",
    "print(\"best score : \",grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93333333333333335"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = grid.best_estimator_\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Learning data using SVC Model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We will repeat the whole process, and the only difference is the hyperparamaters given to the gridSearch instance.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=1, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'kernel': ['rbf'], 'gamma': [0.001, 0.0001], 'C': [1, 10, 100, 1000]}, {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# choose possible paramaters\n",
    "param_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "# instantiate the grid\n",
    "# the cv param indicates the number of folds and the scoring param indicates the scoring stratetgy\n",
    "grid = GridSearchCV(SVC(random_state=1), param_grid, cv=10, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "\n",
    "# fit the grid with data\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best paramaters :  {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "best score :  0.98\n"
     ]
    }
   ],
   "source": [
    "# print the best hyper-paramaters\n",
    "print(\"best paramaters : \" ,grid.best_params_)\n",
    "print(\"best score : \",grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96666666666666667"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = grid.best_estimator_\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
